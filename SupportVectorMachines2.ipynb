{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbf83f35-cf56-4d6c-bbf4-4abae4cfccc0",
   "metadata": {},
   "source": [
    "Q1. What is the relationship between polynomial functions and kernel functions in machine learning\n",
    "algorithms?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d03403-f3ed-48e3-9be9-22e25948f32e",
   "metadata": {},
   "source": [
    "Polynomial functions and kernel functions are related in machine learning algorithms, particularly in algorithms like Support Vector Machines (SVMs) and Kernel Ridge Regression (KRR). The relationship lies in how kernel functions can implicitly represent polynomial transformations of input features without explicitly computing them.\n",
    "\n",
    "1. **Polynomial Functions**:\n",
    "   - Polynomial functions are mathematical functions of the form \\( f(x) = a_n x^n + a_{n-1} x^{n-1} + \\ldots + a_1 x + a_0 \\), where \\( x \\) is the input variable, and \\( a_n, a_{n-1}, \\ldots, a_0 \\) are coefficients.\n",
    "   - In machine learning, polynomial functions are used to model non-linear relationships between features. They can capture curved or non-linear decision boundaries.\n",
    "\n",
    "2. **Kernel Functions**:\n",
    "   - Kernel functions in machine learning, such as the polynomial kernel, are similarity functions that compute the inner product or similarity between data points in a high-dimensional feature space.\n",
    "   - The polynomial kernel is defined as \\( K(x_i, x_j) = (x_i^T x_j + c)^d \\), where \\( d \\) is the degree of the polynomial and \\( c \\) is a constant.\n",
    "   - The polynomial kernel allows SVMs and other algorithms to operate in a higher-dimensional feature space without explicitly computing the transformation of input features into that space.\n",
    "\n",
    "3. **Relationship**:\n",
    "   - The relationship between polynomial functions and kernel functions lies in the fact that certain kernel functions, like the polynomial kernel, can implicitly represent polynomial transformations of input features.\n",
    "   - Instead of explicitly transforming the input features into a higher-dimensional space using polynomial functions (which can be computationally expensive, especially for high degrees), kernel functions compute the dot product directly in the original space but in a higher-dimensional feature space.\n",
    "   - This allows machine learning algorithms to effectively learn non-linear decision boundaries without the need to compute the transformed features explicitly.\n",
    "\n",
    "In summary, polynomial functions and kernel functions are related in the sense that kernel functions, such as the polynomial kernel, can represent polynomial transformations of input features in a higher-dimensional space without actually performing the transformation explicitly. This relationship is fundamental in enabling algorithms like SVMs to handle non-linear relationships and capture complex decision boundaries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361eec0a-37c2-4989-81fe-d9208cc353fa",
   "metadata": {},
   "source": [
    "Q2. How can we implement an SVM with a polynomial kernel in Python using Scikit-learn?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd401282-6c37-4cfb-910f-4e76262cf3ac",
   "metadata": {},
   "source": [
    "You can implement an SVM with a polynomial kernel in Python using Scikit-learn's SVC (Support Vector Classifier) class. The polynomial kernel is specified using the kernel='poly' parameter in the SVC constructor. You can also adjust other parameters such as the degree of the polynomial, the regularization parameter C, and the coefficient gamma.\n",
    "\n",
    "Here's an example of implementing an SVM with a polynomial kernel using Scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4fcfa5a7-578f-4615-8608-c37f91fcb7c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of SVM with polynomial kernel: 1.00\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize an SVM classifier with a polynomial kernel\n",
    "svm_classifier = SVC(kernel='poly', degree=3, C=1.0, gamma='scale')  # Adjust parameters as needed\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict labels for the testing set\n",
    "y_pred = svm_classifier.predict(X_test)\n",
    "\n",
    "# Compute accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy of SVM with polynomial kernel: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b4960b-0398-4bd1-a4e0-d47d423d1505",
   "metadata": {},
   "source": [
    "Q3. How does increasing the value of epsilon affect the number of support vectors in SVR?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e30dd1e-eb5a-4224-bc75-367c98e5930f",
   "metadata": {},
   "source": [
    "In Support Vector Regression (SVR), the parameter \\( \\epsilon \\) determines the width of the tube around the regression line within which no penalty is given for errors. This parameter has an impact on the number of support vectors used by the SVR model.\n",
    "\n",
    "- **Decreasing \\( \\epsilon \\):**\n",
    "  - When \\( \\epsilon \\) is decreased, the tube around the regression line becomes narrower.\n",
    "  - A narrower tube means that fewer data points can lie within the tube without incurring a penalty, as errors are not tolerated as much.\n",
    "  - This can lead to a smaller number of support vectors since the model becomes stricter in fitting the data points within a tighter tolerance.\n",
    "\n",
    "- **Increasing \\( \\epsilon \\):**\n",
    "  - Conversely, when \\( \\epsilon \\) is increased, the tube around the regression line becomes wider.\n",
    "  - A wider tube allows more data points to lie within the tube without incurring a penalty, providing more flexibility in fitting the data.\n",
    "  - This can lead to a larger number of support vectors since the model can accommodate more data points within the wider tolerance.\n",
    "\n",
    "In summary, increasing the value of \\( \\epsilon \\) generally leads to an increase in the number of support vectors, as the model becomes more tolerant of errors and allows more data points to be considered support vectors within the wider margin. Conversely, decreasing \\( \\epsilon \\) results in a smaller number of support vectors as the model becomes stricter and requires data points to be closer to the regression line to avoid penalties."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d112c4a8-2810-49e0-9ae2-26aa7667c3c7",
   "metadata": {},
   "source": [
    "Q4. How does the choice of kernel function, C parameter, epsilon parameter, and gamma parameter\n",
    "affect the performance of Support Vector Regression (SVR)? Can you explain how each parameter works\n",
    "and provide examples of when you might want to increase or decrease its value?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b009472-dc1e-4c6c-a100-1043a99fe188",
   "metadata": {},
   "source": [
    "Certainly! Let's discuss how each parameter in Support Vector Regression (SVR) affects the model's performance and provide examples of when you might want to increase or decrease its value.\n",
    "\n",
    "1. **Kernel Function**:\n",
    "   - The choice of kernel function in SVR affects the model's ability to capture complex relationships between features.\n",
    "   - Common kernel functions include linear, polynomial, radial basis function (RBF), and sigmoid.\n",
    "   - **Example**: Use a polynomial kernel when you suspect the relationship between features and target variable has a polynomial form. Use an RBF kernel for non-linear relationships without a specific polynomial form.\n",
    "\n",
    "2. **C Parameter**:\n",
    "   - The C parameter controls the trade-off between the model's complexity (flexibility) and the amount of error tolerated.\n",
    "   - A smaller C leads to a smoother decision boundary with more errors tolerated, while a larger C leads to a more complex decision boundary with fewer errors tolerated.\n",
    "   - **Example**: Increase C when you have a smaller dataset or want a more complex model. Decrease C when you have a larger dataset or want a simpler, more generalizable model.\n",
    "\n",
    "3. **Epsilon Parameter**:\n",
    "   - The epsilon parameter (also known as the epsilon-insensitive loss parameter) determines the width of the tube around the regression line within which no penalty is given for errors.\n",
    "   - A smaller epsilon results in a narrower tube, while a larger epsilon results in a wider tube.\n",
    "   - **Example**: Increase epsilon if you want the model to be more tolerant of errors and have a wider margin. Decrease epsilon if you want the model to be stricter and have a narrower margin.\n",
    "\n",
    "4. **Gamma Parameter**:\n",
    "   - The gamma parameter affects the influence of individual training samples on the model's decision boundary in non-linear kernels like RBF.\n",
    "   - A smaller gamma leads to a smoother decision boundary, while a larger gamma leads to a more complex decision boundary with more emphasis on individual data points.\n",
    "   - **Example**: Increase gamma when you want the model to closely fit training data points, possibly leading to overfitting. Decrease gamma to prevent overfitting and promote generalization.\n",
    "\n",
    "**Parameter Adjustment Examples**:\n",
    "- **Increasing C**: If your training data is small and complex, increasing C can help the model capture intricate patterns.\n",
    "- **Decreasing C**: For a large dataset or when aiming for a simpler model, decreasing C can prevent overfitting and improve generalization.\n",
    "- **Increasing Epsilon**: When you're comfortable with a wider margin of error or want the model to be more robust to noise, increasing epsilon can be beneficial.\n",
    "- **Decreasing Epsilon**: When you want the model to closely follow the training data, decreasing epsilon helps in creating a narrower margin.\n",
    "- **Increasing Gamma**: In non-linear kernels like RBF, increasing gamma can lead to a more detailed fit to the training data, which may be desirable if the data is complex but risks overfitting.\n",
    "- **Decreasing Gamma**: Decreasing gamma helps in creating a smoother decision boundary, which can prevent overfitting and promote better generalization.\n",
    "\n",
    "Adjusting these parameters requires a good understanding of the data, model complexity, and the trade-offs between model flexibility, generalization, and overfitting. It often involves experimentation and tuning using techniques like grid search or randomized search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba5e5bc-629c-4225-a4cb-b63385b9deff",
   "metadata": {},
   "source": [
    "Q5. Assignment:\n",
    "L Import the necessary libraries and load the dataseg                \n",
    "L Split the dataset into training and testing setZ                                      \n",
    "L Preprocess the data using any technique of your choice (e.g. scaling, normaliMationK              \n",
    "L Create an instance of the SVC classifier and train it on the training datW                    \n",
    "L hse the trained classifier to predict the labels of the testing datW                              \n",
    "L Evaluate the performance of the classifier using any metric of your choice (e.g. accuracy,\n",
    "precision, recall, F1-scoreK                                                                   \n",
    "L Tune the hyperparameters of the SVC classifier using GridSearchCV or RandomiMedSearchCV to\n",
    "improve its performanc_                                \n",
    "L Train the tuned classifier on the entire dataseg                    \n",
    "L Save the trained classifier to a file for future use.                                           \n",
    "\n",
    "You can use any dataset of your choice for this assignment, but make sure it is suitable for\n",
    "classification and has a sufficient number of features and samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d1e1342-6751-4581-942a-1fa72291802b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import joblib\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c31ab41-414c-4fc0-babf-87f1db807aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "656126ce-763a-49e6-955f-938ac22be20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df7e2e42-709d-480b-b6f2-bb38a811ddca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC()"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc_clf = SVC()\n",
    "svc_clf.fit(X_train_scaled, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9af92b11-b808-4752-a048-e93a0cb9a40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = svc_clf.predict(X_test_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "afa8a5c5-b716-4272-88f1-334024a993a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        10\n",
      "           1       1.00      1.00      1.00         9\n",
      "           2       1.00      1.00      1.00        11\n",
      "\n",
      "    accuracy                           1.00        30\n",
      "   macro avg       1.00      1.00      1.00        30\n",
      "weighted avg       1.00      1.00      1.00        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\\n\", report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b2a9034-b484-49e7-ba76-6ccb58d363b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'C': 10, 'gamma': 0.1, 'kernel': 'linear'}\n"
     ]
    }
   ],
   "source": [
    "param_grid = {'C': [0.1, 1, 10, 100], 'gamma': [0.1, 1, 10, 100], 'kernel': ['rbf', 'linear']}\n",
    "grid_search = GridSearchCV(SVC(), param_grid, cv=5)\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "52fc8887-6815-4f20-bbee-52adeb04056b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(C=10, gamma=0.1, kernel=&#x27;linear&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(C=10, gamma=0.1, kernel=&#x27;linear&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(C=10, gamma=0.1, kernel='linear')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuned_svc_clf = grid_search.best_estimator_\n",
    "tuned_svc_clf.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "34fd57cd-6b8e-4d89-9d8a-989e9f0899e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tuned_svc_classifier.pkl']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(tuned_svc_clf, 'tuned_svc_classifier.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545621d2-e93f-4995-84d4-1ae182ec9d13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
