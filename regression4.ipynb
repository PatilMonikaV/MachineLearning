{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d83ac7f1-a563-4db0-bd04-f6b336a83efe",
   "metadata": {},
   "source": [
    "Q1. What is Lasso Regression, and how does it differ from other regression techniques?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7526de0e-88d9-4c1c-a002-1d3c4bc138fb",
   "metadata": {},
   "source": [
    "Lasso Regression, short for Least Absolute Shrinkage and Selection Operator Regression, is a linear regression technique that incorporates regularization to improve the model's performance, especially in high-dimensional datasets with multicollinearity. Here's a breakdown of Lasso Regression and its differences from other regression techniques:\n",
    "\n",
    "### 1. Lasso Regression Overview\n",
    "- **Regularization:** Lasso Regression adds a penalty term to the standard linear regression cost function, which penalizes large coefficients. The penalty term is proportional to the absolute value of the coefficients (\\(\\beta_j\\)), leading to sparsity in the coefficient vector.\n",
    "  \n",
    "- **Feature Selection:** One of the key features of Lasso Regression is its ability to perform automatic feature selection by setting some coefficients to exactly zero. This is particularly useful when dealing with datasets with many features, as it helps identify the most relevant predictors.\n",
    "\n",
    "### 2. Differences from Other Regression Techniques\n",
    "\n",
    "#### a. Lasso vs. Ridge Regression\n",
    "- **Penalty Type:** Lasso Regression uses an L1 regularization penalty (\\(\\lambda \\sum_{j=1}^{p} |\\beta_j|\\)), while Ridge Regression uses an L2 regularization penalty (\\(\\lambda \\sum_{j=1}^{p} \\beta_j^2\\)).\n",
    "  \n",
    "- **Coefficient Shrinkage:** Lasso Regression tends to shrink some coefficients all the way to zero, effectively performing variable selection. In contrast, Ridge Regression only shrinks coefficients towards zero but does not set them exactly to zero.\n",
    "\n",
    "- **Sparsity:** Lasso Regression encourages sparsity in the coefficient vector, leading to a more parsimonious model with fewer features.\n",
    "\n",
    "#### b. Lasso vs. Elastic Net Regression\n",
    "- **Penalty Composition:** Elastic Net Regression combines L1 (Lasso) and L2 (Ridge) penalties in its regularization term, allowing for both variable selection and handling multicollinearity.\n",
    "\n",
    "- **Flexibility:** Elastic Net is more flexible than Lasso Regression because it allows the inclusion of correlated predictors without completely eliminating them (as Lasso might).\n",
    "\n",
    "- **Parameter Tuning:** Elastic Net introduces an additional hyperparameter (\\(r\\)) to control the balance between L1 and L2 penalties, providing more control over the regularization process.\n",
    "\n",
    "#### c. Lasso vs. Ordinary Least Squares (OLS) Regression\n",
    "- **Regularization:** Lasso Regression adds a regularization term to the OLS cost function, penalizing large coefficients. OLS Regression does not include any regularization and may lead to overfitting, especially in high-dimensional datasets.\n",
    "\n",
    "- **Feature Selection:** Lasso Regression can perform feature selection by setting some coefficients to zero, whereas OLS Regression uses all features without selection.\n",
    "\n",
    "- **Bias-Variance Trade-off:** Lasso Regression introduces a controlled amount of bias to reduce variance, which helps improve model generalization.\n",
    "\n",
    "### Advantages of Lasso Regression\n",
    "1. **Feature Selection:** Automatically selects relevant features by setting irrelevant ones to zero, which can improve model interpretability and reduce overfitting.\n",
    "2. **Handles Multicollinearity:** Can handle multicollinearity by choosing one variable from a group of highly correlated variables.\n",
    "3. **Simplicity:** Provides a simpler model with fewer features, which can be advantageous in high-dimensional datasets.\n",
    "\n",
    "### Limitations of Lasso Regression\n",
    "1. **Unstable with Correlated Predictors:** Lasso Regression may arbitrarily choose one variable over another if they are highly correlated, leading to instability in coefficient selection.\n",
    "2. **Sensitive to Scaling:** Lasso is sensitive to the scale of predictors, so feature scaling (e.g., standardization) is often necessary.\n",
    "3. **Not Suitable for Non-Sparse Solutions:** In cases where a dense solution is needed (all predictors are important), Lasso Regression may not be appropriate due to its tendency to create sparse solutions.\n",
    "\n",
    "Overall, Lasso Regression is a powerful technique for feature selection, regularization, and handling high-dimensional datasets, but it requires careful consideration of its limitations and appropriate tuning of hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8c8905-0103-4931-82ea-6227ea2abb2c",
   "metadata": {},
   "source": [
    "Q2. What is the main advantage of using Lasso Regression in feature selection?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5518f131-035a-4324-b824-efd198495f26",
   "metadata": {},
   "source": [
    "The main advantage of using Lasso Regression in feature selection is its ability to automatically select relevant features while effectively discarding irrelevant or redundant ones. This feature selection capability offers several benefits:\n",
    "\n",
    "### 1. Automatic Feature Selection\n",
    "Lasso Regression performs automatic feature selection by shrinking some coefficients to exactly zero. This means that Lasso can effectively eliminate features that have little or no impact on the target variable from the model. By setting certain coefficients to zero, Lasso identifies and selects only the most important predictors, leading to a more parsimonious and interpretable model.\n",
    "\n",
    "### 2. Improved Model Interpretability\n",
    "With fewer features in the model due to feature selection, the interpretation of the model becomes easier. You can focus on the selected features and their coefficients, understanding their impact on the target variable without the complexity of irrelevant or redundant features. This improves the model's interpretability, making it more accessible to stakeholders and aiding in decision-making processes.\n",
    "\n",
    "### 3. Reduced Overfitting\n",
    "Feature selection through Lasso Regression helps mitigate overfitting, especially in high-dimensional datasets where the number of features is much larger than the number of observations. By discarding irrelevant features, Lasso reduces the complexity of the model and prevents it from learning noise or capturing spurious relationships present in the data. This leads to better generalization performance on unseen data.\n",
    "\n",
    "### 4. Handling Multicollinearity\n",
    "Lasso Regression can effectively handle multicollinearity, which occurs when predictors are highly correlated with each other. By selecting one feature from a group of correlated features and setting others to zero, Lasso deals with multicollinearity issues and prevents the model from being overly sensitive to small changes in the data.\n",
    "\n",
    "### 5. Computational Efficiency\n",
    "In scenarios with a large number of features, using Lasso Regression for feature selection can improve computational efficiency. Since Lasso shrinks coefficients to zero, it effectively reduces the dimensionality of the problem, making computations faster and more manageable compared to models with all features included.\n",
    "\n",
    "### Example Scenario\n",
    "Imagine you're working on a dataset with hundreds of features, including some that are highly correlated or redundant. By applying Lasso Regression for feature selection, you can automatically identify and keep only the most relevant features while discarding the rest. This not only simplifies the model but also improves its predictive performance and interpretability.\n",
    "\n",
    "Overall, the main advantage of using Lasso Regression in feature selection is its ability to create simpler, more interpretable models with improved generalization performance by automatically identifying and retaining important features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545959f4-9f17-4b78-aa8a-ec5b69f891cf",
   "metadata": {},
   "source": [
    "Q3. How do you interpret the coefficients of a Lasso Regression model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43dd864a-77fa-4906-a6e6-3b25aecc1e03",
   "metadata": {},
   "source": [
    "Interpreting the coefficients of a Lasso Regression model involves understanding the effects of regularization on the coefficients and their implications for feature selection. Here's how you can interpret Lasso Regression coefficients:\n",
    "\n",
    "### 1. Coefficient Sign and Magnitude\n",
    "- **Sign:** The sign of a coefficient (\\(\\beta_j\\)) indicates the direction of the relationship between the corresponding feature and the target variable. A positive coefficient suggests a positive impact on the target when the feature increases, while a negative coefficient suggests an inverse relationship.\n",
    "  \n",
    "- **Magnitude:** The magnitude of a coefficient reflects the strength of the relationship. Larger absolute values indicate stronger impact, while smaller values indicate weaker impact.\n",
    "\n",
    "### 2. Effect of Lasso Regularization\n",
    "Lasso Regression adds an L1 regularization term to the cost function, penalizing large coefficients. This penalty term encourages sparsity in the coefficient vector by pushing some coefficients to exactly zero. As a result:\n",
    "  \n",
    "- **Non-Zero Coefficients:** Features with non-zero coefficients in a Lasso model are considered relevant and have an impact on predictions. These features contribute to the model's predictive power.\n",
    "\n",
    "- **Zero Coefficients:** Features with coefficients set to zero are effectively excluded from the model. Lasso Regression performs feature selection by automatically removing irrelevant or less important features, leading to a more interpretable and parsimonious model.\n",
    "\n",
    "### 3. Feature Importance and Selection\n",
    "- **Non-Zero Coefficients:** Interpret non-zero coefficients as indicators of feature importance. Features with larger non-zero coefficients have a stronger impact on predictions and are more influential in explaining variation in the target variable.\n",
    "\n",
    "- **Zero Coefficients:** Features with coefficients set to zero are effectively deemed unimportant or redundant by Lasso Regression. These features do not contribute significantly to the model's predictive power and can be considered as excluded from the model's decision-making process.\n",
    "\n",
    "### 4. Example Interpretation\n",
    "Consider a Lasso Regression model for predicting housing prices with features like size, number of bedrooms, and location. After fitting the model, you observe the following coefficients:\n",
    "\n",
    "- Size: 10.2\n",
    "- Bedrooms: 5.8\n",
    "- Location (Downtown): 0.0\n",
    "- Location (Suburb): 2.1\n",
    "\n",
    "Interpretation:\n",
    "- Size and bedrooms have non-zero coefficients, indicating their importance in predicting prices. An increase in size or bedrooms leads to a corresponding increase in predicted prices.\n",
    "  \n",
    "- The coefficient for Downtown location is exactly zero, suggesting that this feature (e.g., being in downtown) has no impact on prices in this model. It has been effectively excluded from the model's predictions.\n",
    "\n",
    "### Summary\n",
    "- **Sign and Magnitude:** Coefficients' signs and magnitudes reflect the direction and strength of relationships between features and the target variable.\n",
    "- **Effect of Lasso Regularization:** Lasso penalizes large coefficients and encourages sparsity, leading to feature selection.\n",
    "- **Feature Importance:** Non-zero coefficients indicate feature importance, while zero coefficients indicate excluded features.\n",
    "- **Interpretation Challenges:** Interpretation of Lasso coefficients should consider regularization effects, feature selection, and the overall context of the model and dataset.\n",
    "\n",
    "When interpreting Lasso Regression coefficients, it's essential to keep in mind the regularization effects and the resulting feature selection, which contribute to the model's simplicity and interpretability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50f8947-10b8-48cd-b2cd-1bd13497aea0",
   "metadata": {},
   "source": [
    "Q4. What are the tuning parameters that can be adjusted in Lasso Regression, and how do they affect the\n",
    "model's performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa5e276-f981-4ca2-b01e-f72d6860f129",
   "metadata": {},
   "source": [
    "In Lasso Regression, there are primarily two tuning parameters that can be adjusted to control the model's behavior and performance:\n",
    "\n",
    "1. **Alpha (\\(\\alpha\\)):**\n",
    "   - Alpha is the regularization parameter in Lasso Regression. It determines the strength of the regularization penalty applied to the coefficients. Higher values of alpha result in stronger regularization, leading to more coefficients being pushed towards zero and potentially more features being excluded from the model.\n",
    "   - **Effect on Model's Performance:** Adjusting alpha allows you to control the bias-variance trade-off in the model. Higher alpha values increase bias but reduce variance by simplifying the model and preventing overfitting. Lower alpha values decrease bias but may increase variance, potentially leading to overfitting in high-dimensional datasets.\n",
    "\n",
    "2. **Max Iterations:**\n",
    "   - Max iterations (max_iter) is the maximum number of iterations or optimization steps allowed during the model's training. It is relevant because Lasso Regression is solved using iterative optimization algorithms like coordinate descent.\n",
    "   - **Effect on Model's Performance:** Increasing max_iter allows the optimization algorithm more iterations to converge to the optimal solution, which can improve model performance, especially for complex or large-scale datasets. However, setting max_iter too high may increase computational time without significant improvements if the model has already converged.\n",
    "\n",
    "Adjusting these tuning parameters requires careful consideration and often involves using techniques like cross-validation to find the optimal values for alpha and max_iter based on the specific dataset and modeling goals. Choosing appropriate tuning parameters is crucial for achieving a well-balanced Lasso Regression model with good predictive performance and interpretability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da5da25-65bd-4d4e-9f59-3f8b016010d4",
   "metadata": {},
   "source": [
    "Q5. Can Lasso Regression be used for non-linear regression problems? If yes, how?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d38689-e6f6-4977-9f54-19890e2a4aab",
   "metadata": {},
   "source": [
    "Lasso Regression is inherently a linear regression technique, meaning it models the relationship between the independent variables and the dependent variable as a linear combination of the predictors. However, Lasso Regression can still be used in conjunction with techniques to handle non-linear regression problems through feature engineering or transformations. Here's how Lasso Regression can be adapted for non-linear regression problems:\n",
    "\n",
    "1. Feature Engineering:\n",
    "Polynomial Features: One common approach is to create polynomial features from the original predictors. By including polynomial terms (e.g., quadratic, cubic) in the feature space, you can capture non-linear relationships between variables. Lasso Regression can then be applied to the expanded feature set.\n",
    "\n",
    "Interaction Terms: Including interaction terms (products of variables) can also introduce non-linear relationships into the model. For example, in a housing price prediction model, an interaction term between size and number of bedrooms could capture non-linear effects.\n",
    "\n",
    "2. Transformations:\n",
    "Logarithmic Transformation: If the relationship between predictors and the target variable is non-linear and exhibits diminishing returns or exponential growth, applying logarithmic transformations to the predictors or the target variable can help linearize the relationship.\n",
    "\n",
    "Box-Cox Transformation: The Box-Cox transformation is a more general transformation that can handle a wider range of non-linearities by optimizing a power transformation parameter. After transformation, Lasso Regression can be applied to the transformed data.\n",
    "\n",
    "3. Kernel Methods:\n",
    "Kernel Tricks: In some cases, kernel methods such as the kernel trick in Support Vector Machines (SVM) can be adapted for non-linear regression. While Lasso Regression itself does not inherently support kernel methods, you can preprocess the data using kernel transformations before applying Lasso Regression.\n",
    "Example Code (Python with Scikit-Learn):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53ea134a-7539-416e-a1c6-521274fbbd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Generate synthetic non-linear data\n",
    "X = np.linspace(-5, 5, 100).reshape(-1, 1)\n",
    "y = 2*X**3 - X**2 + 3*X + np.random.normal(0, 3, size=X.shape[0])\n",
    "\n",
    "# Create a pipeline with polynomial features and Lasso Regression\n",
    "pipeline = Pipeline([\n",
    "    ('poly_features', PolynomialFeatures(degree=3)),  # Use polynomial features up to degree 3\n",
    "    ('lasso', Lasso(alpha=0.1))  # Lasso Regression with chosen alpha\n",
    "])\n",
    "\n",
    "# Fit the model\n",
    "pipeline.fit(X, y)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = pipeline.predict(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a72908-09fe-4d70-a739-acbf25f9906a",
   "metadata": {},
   "source": [
    "Q6. What is the difference between Ridge Regression and Lasso Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f70715-19c2-4166-8768-2981c85fd8ec",
   "metadata": {},
   "source": [
    "Ridge Regression and Lasso Regression are both linear regression techniques that incorporate regularization to improve model performance and address issues like multicollinearity and overfitting. However, they differ primarily in the type of regularization they apply and the resulting behavior in terms of feature selection and coefficient shrinkage. Here are the key differences between Ridge Regression and Lasso Regression:\n",
    "\n",
    "### 1. Regularization Penalty:\n",
    "- **Ridge Regression:**\n",
    "  - **Penalty Type:** Uses an L2 regularization penalty, which adds the squared magnitude of coefficients to the cost function: \\(\\lambda \\sum_{j=1}^{p} \\beta_j^2\\).\n",
    "  - **Effect:** Encourages small but non-zero coefficients, leading to shrinkage of all coefficients towards zero. Coefficients are reduced but not set to exactly zero.\n",
    "\n",
    "- **Lasso Regression:**\n",
    "  - **Penalty Type:** Uses an L1 regularization penalty, which adds the absolute magnitude of coefficients to the cost function: \\(\\lambda \\sum_{j=1}^{p} |\\beta_j|\\).\n",
    "  - **Effect:** Encourages sparsity by setting some coefficients exactly to zero. This leads to feature selection, where irrelevant or less important features are eliminated from the model.\n",
    "\n",
    "### 2. Coefficient Shrinkage:\n",
    "- **Ridge Regression:** \n",
    "  - Shrinks coefficients towards zero but does not set them exactly to zero. \n",
    "  - Suitable for situations where all predictors may be relevant or correlated with the target, and a balance between bias and variance is desired.\n",
    "\n",
    "- **Lasso Regression:** \n",
    "  - Shrinks coefficients towards zero and can set some coefficients exactly to zero.\n",
    "  - Particularly effective for feature selection, as it automatically identifies and excludes irrelevant or less important features.\n",
    "\n",
    "### 3. Sparsity and Feature Selection:\n",
    "- **Ridge Regression:** \n",
    "  - Does not inherently perform feature selection, as all predictors typically have non-zero coefficients (although small).\n",
    "  - Can handle multicollinearity by shrinking correlated coefficients.\n",
    "\n",
    "- **Lasso Regression:** \n",
    "  - Performs automatic feature selection by setting some coefficients to zero.\n",
    "  - Suitable for high-dimensional datasets with many predictors, as it simplifies the model and improves interpretability.\n",
    "\n",
    "### 4. Stability and Interpretability:\n",
    "- **Ridge Regression:** \n",
    "  - Generally more stable when predictors are highly correlated.\n",
    "  - Maintains all predictors in the model, which can be advantageous in certain scenarios.\n",
    "\n",
    "- **Lasso Regression:** \n",
    "  - May exhibit instability when predictors are highly correlated, as it arbitrarily selects one feature over another.\n",
    "  - Produces a more interpretable model with fewer predictors, which can aid in understanding and explaining the model's behavior.\n",
    "\n",
    "### 5. Computational Complexity:\n",
    "- **Ridge Regression:** \n",
    "  - Typically computationally efficient, as it involves solving a linear system with a unique solution.\n",
    "  \n",
    "- **Lasso Regression:** \n",
    "  - Slightly more computationally intensive due to the absolute value penalty in the optimization process. However, efficient algorithms like coordinate descent are available for optimization.\n",
    "\n",
    "### Summary:\n",
    "- Ridge Regression and Lasso Regression differ in their regularization penalties and resulting behavior regarding coefficient shrinkage and feature selection.\n",
    "- Ridge Regression is suitable for reducing multicollinearity and balancing bias-variance trade-off, while Lasso Regression excels in automatic feature selection and producing sparse models.\n",
    "- The choice between Ridge and Lasso Regression depends on the specific dataset, the importance of feature selection, and the desired interpretability of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed4e964-0a7c-4efe-bb33-7ec121fbc1e8",
   "metadata": {},
   "source": [
    "Q7. Can Lasso Regression handle multicollinearity in the input features? If yes, how?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25fb884d-2a66-4286-ba71-200fddaa7af8",
   "metadata": {},
   "source": [
    "Yes, Lasso Regression can handle multicollinearity in the input features, although its approach differs from that of Ridge Regression. Multicollinearity occurs when predictors in a regression model are highly correlated with each other, which can lead to unstable coefficient estimates and difficulties in interpreting the model. Lasso Regression addresses multicollinearity through feature selection, which is one of its key advantages.\n",
    "\n",
    "Here's how Lasso Regression handles multicollinearity:\n",
    "\n",
    "1. **Feature Selection:**\n",
    "   - Lasso Regression automatically performs feature selection by shrinking some coefficients to exactly zero. When predictors are highly correlated (multicollinear), Lasso tends to select one predictor from the group of correlated predictors and sets the coefficients of the others to zero.\n",
    "   - By excluding irrelevant or redundant features with zero coefficients, Lasso effectively deals with multicollinearity issues. This feature selection process simplifies the model and improves its interpretability.\n",
    "\n",
    "2. **Encouraging Sparsity:**\n",
    "   - The L1 regularization penalty in Lasso Regression encourages sparsity in the coefficient vector. The penalty term \\(\\lambda \\sum_{j=1}^{p} |\\beta_j|\\) promotes smaller coefficients and sets some of them to zero, favoring a sparse solution.\n",
    "   - In the context of multicollinearity, Lasso's sparsity-inducing property helps in selecting a subset of predictors that are most informative for predicting the target variable, while disregarding redundant or less important predictors.\n",
    "\n",
    "3. **Impact on Coefficient Estimates:**\n",
    "   - When multicollinearity is present, Lasso Regression may lead to more stable and interpretable coefficient estimates compared to ordinary least squares (OLS) regression. This is because Lasso selects a subset of predictors and assigns them non-zero coefficients based on their importance, reducing the influence of multicollinear predictors.\n",
    "\n",
    "4. **Trade-off with Ridge Regression:**\n",
    "   - Compared to Ridge Regression, Lasso Regression tends to produce more sparse solutions when dealing with multicollinearity. Ridge Regression also addresses multicollinearity by shrinking coefficients but does not perform feature selection by setting coefficients exactly to zero.\n",
    "\n",
    "However, it's important to note that Lasso Regression may exhibit some limitations in handling multicollinearity, especially when predictors are highly correlated. In such cases, Lasso may arbitrarily select one predictor over another from the group of correlated predictors, potentially leading to instability or sensitivity to small changes in the data.\n",
    "\n",
    "Overall, while Lasso Regression offers effective mechanisms for handling multicollinearity through feature selection and sparsity, careful consideration of the dataset and validation techniques is necessary to ensure the stability and reliability of the model's results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f385f86-af29-4c8a-9381-7d3ea6f728a3",
   "metadata": {},
   "source": [
    "Q8. How do you choose the optimal value of the regularization parameter (lambda) in Lasso Regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b867c2f-a9d1-4c28-a9f1-aebb6cb609eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e47684b-2823-44fc-84fb-67ebfd8d3106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha: 0.1\n",
      "Train R^2: 0.5169, Test R^2: 0.4719\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load data\n",
    "diabetes = load_diabetes()\n",
    "X_train, X_test, y_train, y_test = train_test_split(diabetes.data, diabetes.target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define Lasso Regression model\n",
    "lasso = Lasso()\n",
    "\n",
    "# Define range of alpha values (lambda) to search\n",
    "alphas = [0.01, 0.1, 1.0, 10.0]\n",
    "\n",
    "# Perform grid search cross-validation\n",
    "grid_search = GridSearchCV(estimator=lasso, param_grid={'alpha': alphas}, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get best alpha value\n",
    "best_alpha = grid_search.best_params_['alpha']\n",
    "print(f\"Best alpha: {best_alpha}\")\n",
    "\n",
    "# Fit Lasso Regression with best alpha\n",
    "lasso_best = Lasso(alpha=best_alpha)\n",
    "lasso_best.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate model performance\n",
    "train_score = lasso_best.score(X_train, y_train)\n",
    "test_score = lasso_best.score(X_test, y_test)\n",
    "print(f\"Train R^2: {train_score:.4f}, Test R^2: {test_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ebd66d-bcc1-4281-8182-39519e48d156",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
