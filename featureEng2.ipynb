{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0af7cb2a-d99e-4956-b710-f9863f7dd407",
   "metadata": {},
   "source": [
    "Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its\n",
    "application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3bafe1-14f0-48ed-8373-22ab045029df",
   "metadata": {},
   "source": [
    "Min-Max scaling, also known as normalization, is a data preprocessing technique that rescales numerical features to a specific range, typically between 0 and 1. It's used to ensure that features measured at different scales contribute equally to model fitting. \n",
    "\n",
    " Min-max scaling, also known as normalization, is a technique commonly used in data preprocessing. It is used to transform numerical features into a specific range, typically between 0 and 1. Min-max scaling can be useful in various situations, such as: Machine Learning Algorithms: Many machine learning algorithms ...\n",
    "\n",
    "What is Min-Max Scaler? Min-Max Scaler, also known as Normalization, is a data scaling technique that transforms numerical features to a desired range, typically between 0 and 1. It achieves this by subtracting the minimum value and dividing by the range (the difference between the maximum and minimum values).\n",
    "How it works      \n",
    "Min-Max scaling subtracts the minimum value from each feature and then divides by the range (maximum value minus minimum value). The formula is:\n",
    "X_std = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))       \n",
    "Example        \n",
    "If a series of album ratings range from 70 to 150, Min-Max scaling can be used to rescale them so that all ratings fall between 0 and 1. This preserves the proportional distance between data points.      \n",
    "Benefits     \n",
    "Min-Max scaling can improve the stability and speed of back-propagation in multi-layer perceptrons (MLPs).    \n",
    "It works well when the standard deviation is small or the data doesn't have a Gaussian distribution.       \n",
    "Limitations     \n",
    "Min-Max scaling is sensitive to outliers, as it linearly scales them down into a fixed range. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff9659c-448e-4763-a732-9262724704dd",
   "metadata": {},
   "source": [
    "Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling?\n",
    "Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b57104-7fe1-452a-8cbc-e2ac13403a6c",
   "metadata": {},
   "source": [
    "The Unit Vector technique, also known as normalization to a unit norm, is a feature scaling method that scales each data point (or vector) to have a unit norm (length of 1). This technique is useful when the direction of the data is more important than its magnitude.\n",
    "\n",
    "### How the Unit Vector Technique Works\n",
    "\n",
    "The scaling is performed using the following formula:\n",
    "\n",
    "\\[ X_{\\text{scaled}} = \\frac{X}{\\|X\\|} \\]\n",
    "\n",
    "where:\n",
    "- \\( X \\) is the original feature vector.\n",
    "- \\( \\|X\\| \\) is the norm (length) of the vector \\( X \\), commonly the Euclidean norm given by \\( \\|X\\| = \\sqrt{\\sum_{i=1}^{n} X_i^2} \\).\n",
    "\n",
    "### Difference from Min-Max Scaling\n",
    "\n",
    "- **Min-Max Scaling**: Transforms the values of a feature to a fixed range, typically [0, 1]. It focuses on the range of values in the feature.\n",
    "- **Unit Vector Scaling**: Transforms each vector to have a unit norm, focusing on the direction rather than the magnitude of the data.\n",
    "\n",
    "### Example of the Unit Vector Technique\n",
    "\n",
    "Suppose we have a dataset with two features for three data points as follows:\n",
    "\n",
    "\\[ \n",
    "\\begin{pmatrix}\n",
    "3 & 4 \\\\\n",
    "1 & 2 \\\\\n",
    "0 & 1 \\\\\n",
    "\\end{pmatrix}\n",
    "\\]\n",
    "\n",
    "To apply the Unit Vector technique:\n",
    "\n",
    "1. Calculate the norm for each vector (row):\n",
    "\n",
    "\\[\n",
    "\\|X\\| = \\sqrt{x_1^2 + x_2^2}\n",
    "\\]\n",
    "\n",
    "2. Normalize each vector:\n",
    "\n",
    "\\[\n",
    "\\begin{align*}\n",
    "\\text{Norm of } (3, 4) &= \\sqrt{3^2 + 4^2} = \\sqrt{9 + 16} = 5 \\\\\n",
    "\\text{Norm of } (1, 2) &= \\sqrt{1^2 + 2^2} = \\sqrt{1 + 4} = \\sqrt{5} \\\\\n",
    "\\text{Norm of } (0, 1) &= \\sqrt{0^2 + 1^2} = 1\n",
    "\\end{align*}\n",
    "\\]\n",
    "\n",
    "3. Divide each element by its respective norm:\n",
    "\n",
    "\\[\n",
    "\\begin{pmatrix}\n",
    "\\frac{3}{5} & \\frac{4}{5} \\\\\n",
    "\\frac{1}{\\sqrt{5}} & \\frac{2}{\\sqrt{5}} \\\\\n",
    "0 & 1 \\\\\n",
    "\\end{pmatrix}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "0.6 & 0.8 \\\\\n",
    "0.447 & 0.894 \\\\\n",
    "0 & 1 \\\\\n",
    "\\end{pmatrix}\n",
    "\\]\n",
    "\n",
    "### Applications of the Unit Vector Technique\n",
    "\n",
    "1. **Cosine Similarity**: Frequently used in text analysis and recommendation systems where the focus is on the angle (cosine of the angle) between vectors.\n",
    "2. **Clustering Algorithms**: Algorithms like k-means can benefit from normalized data when the distance metric is sensitive to the scale of the data.\n",
    "3. **Directional Data**: Situations where the orientation of the data is more important than the magnitude, such as in signal processing and computer vision.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201c182d-ac78-40d2-aa89-e3335608c1d8",
   "metadata": {},
   "source": [
    "Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an\n",
    "example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e217568-efac-45d1-bbc5-442c89f690c1",
   "metadata": {},
   "source": [
    "Principal Component Analysis (PCA) is a statistical technique used for dimensionality reduction while preserving as much variability as possible in the dataset. It transforms the original features into a new set of orthogonal features, called principal components, which are ordered by the amount of variance they capture from the data.\n",
    "\n",
    "### How PCA Works\n",
    "\n",
    "1. **Standardization**: Standardize the data to have zero mean and unit variance for each feature.\n",
    "2. **Covariance Matrix Computation**: Compute the covariance matrix to understand how the features vary with respect to each other.\n",
    "3. **Eigenvalue Decomposition**: Compute the eigenvalues and eigenvectors of the covariance matrix. The eigenvectors represent the directions (principal components) and the eigenvalues represent the magnitude of variance along those directions.\n",
    "4. **Principal Components Selection**: Select the top \\( k \\) eigenvectors corresponding to the largest eigenvalues to form a new feature space.\n",
    "5. **Projection**: Project the original data onto the new feature space formed by the selected principal components.\n",
    "\n",
    "### Example of PCA Application\n",
    "\n",
    "Consider a simple example with a dataset consisting of two features:\n",
    "\n",
    "\\[\n",
    "\\begin{pmatrix}\n",
    "2.5 & 2.4 \\\\\n",
    "0.5 & 0.7 \\\\\n",
    "2.2 & 2.9 \\\\\n",
    "1.9 & 2.2 \\\\\n",
    "3.1 & 3.0 \\\\\n",
    "2.3 & 2.7 \\\\\n",
    "2 & 1.6 \\\\\n",
    "1 & 1.1 \\\\\n",
    "1.5 & 1.6 \\\\\n",
    "1.1 & 0.9 \\\\\n",
    "\\end{pmatrix}\n",
    "\\]\n",
    "\n",
    "### Steps to Perform PCA\n",
    "\n",
    "1. **Standardize the Data**:\n",
    "\n",
    "\\[\n",
    "\\text{Standardized Data} =\n",
    "\\begin{pmatrix}\n",
    "0.69 & 0.49 \\\\\n",
    "-1.31 & -1.21 \\\\\n",
    "0.39 & 0.99 \\\\\n",
    "0.09 & 0.29 \\\\\n",
    "1.29 & 1.39 \\\\\n",
    "0.49 & 0.79 \\\\\n",
    "-0.21 & -0.51 \\\\\n",
    "-1.21 & -1.01 \\\\\n",
    "-0.71 & -0.51 \\\\\n",
    "-1.11 & -1.31 \\\\\n",
    "\\end{pmatrix}\n",
    "\\]\n",
    "\n",
    "2. **Compute the Covariance Matrix**:\n",
    "\n",
    "\\[\n",
    "\\text{Covariance Matrix} = \\begin{pmatrix}\n",
    "0.61655556 & 0.61544444 \\\\\n",
    "0.61544444 & 0.71655556 \\\\\n",
    "\\end{pmatrix}\n",
    "\\]\n",
    "\n",
    "3. **Eigenvalue Decomposition**:\n",
    "\n",
    "\\[\n",
    "\\text{Eigenvalues} = \\begin{pmatrix}\n",
    "1.28402771 & 0.0490833989\n",
    "\\end{pmatrix}\n",
    "\\]\n",
    "\n",
    "\\[\n",
    "\\text{Eigenvectors} = \\begin{pmatrix}\n",
    "0.6778734 & -0.73517866 \\\\\n",
    "0.73517866 & 0.6778734 \\\\\n",
    "\\end{pmatrix}\n",
    "\\]\n",
    "\n",
    "4. **Select Principal Components**: Choose the eigenvector corresponding to the largest eigenvalue as the first principal component.\n",
    "\n",
    "\\[\n",
    "\\text{Principal Component} = \\begin{pmatrix}\n",
    "0.6778734 \\\\\n",
    "0.73517866 \\\\\n",
    "\\end{pmatrix}\n",
    "\\]\n",
    "\n",
    "5. **Project the Data**:\n",
    "\n",
    "\\[\n",
    "\\text{Projected Data} = \\text{Standardized Data} \\times \\text{Principal Component}\n",
    "\\]\n",
    "\n",
    "### Result of PCA\n",
    "\n",
    "By projecting the original standardized data onto the principal component, we get a one-dimensional representation of the data that captures most of the variance. This results in a significant reduction of dimensions while preserving the essence of the data's structure.\n",
    "\n",
    "### Applications of PCA\n",
    "\n",
    "1. **Data Compression**: Reducing the number of features while retaining most of the information.\n",
    "2. **Noise Reduction**: Eliminating noise by discarding components with low variance.\n",
    "3. **Visualization**: Projecting high-dimensional data into 2D or 3D for visualization.\n",
    "4. **Feature Extraction**: Identifying the most important features that contribute to data variability.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b53357f-14fa-4be2-89ba-186cad79387a",
   "metadata": {},
   "source": [
    "Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature\n",
    "Extraction? Provide an example to illustrate this concept."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c997d3b8-28d2-4368-9368-a305a695903a",
   "metadata": {},
   "source": [
    "### Relationship Between PCA and Feature Extraction\n",
    "\n",
    "Principal Component Analysis (PCA) is fundamentally related to feature extraction in that it identifies the most important features in a dataset that contribute the most to its variance. While feature extraction typically involves creating new features from the existing ones, PCA does this by transforming the original features into a new set of orthogonal features (principal components). These new features are linear combinations of the original features, and they capture the most significant patterns in the data.\n",
    "\n",
    "### How PCA Can Be Used for Feature Extraction\n",
    "\n",
    "PCA is used for feature extraction by reducing the dimensionality of the data while retaining the most important information. This is done by projecting the original data onto a smaller set of principal components that explain most of the variance.\n",
    "\n",
    "### Example of PCA for Feature Extraction\n",
    "\n",
    "Consider a dataset with three features:\n",
    "\n",
    "\\[\n",
    "\\begin{pmatrix}\n",
    "2.5 & 2.4 & 1.2 \\\\\n",
    "0.5 & 0.7 & 0.8 \\\\\n",
    "2.2 & 2.9 & 1.7 \\\\\n",
    "1.9 & 2.2 & 1.1 \\\\\n",
    "3.1 & 3.0 & 1.6 \\\\\n",
    "2.3 & 2.7 & 1.4 \\\\\n",
    "2.0 & 1.6 & 0.9 \\\\\n",
    "1.0 & 1.1 & 0.5 \\\\\n",
    "1.5 & 1.6 & 0.7 \\\\\n",
    "1.1 & 0.9 & 0.3 \\\\\n",
    "\\end{pmatrix}\n",
    "\\]\n",
    "\n",
    "### Steps to Perform PCA for Feature Extraction\n",
    "\n",
    "1. **Standardize the Data**:\n",
    "\n",
    "\\[\n",
    "\\text{Standardized Data} = \\begin{pmatrix}\n",
    "0.69 & 0.49 & 0.35 \\\\\n",
    "-1.31 & -1.21 & -0.05 \\\\\n",
    "0.39 & 0.99 & 0.85 \\\\\n",
    "0.09 & 0.29 & 0.25 \\\\\n",
    "1.29 & 1.39 & 0.75 \\\\\n",
    "0.49 & 0.79 & 0.55 \\\\\n",
    "-0.21 & -0.51 & -0.05 \\\\\n",
    "-1.21 & -1.01 & -0.45 \\\\\n",
    "-0.71 & -0.51 & -0.25 \\\\\n",
    "-1.11 & -1.31 & -0.65 \\\\\n",
    "\\end{pmatrix}\n",
    "\\]\n",
    "\n",
    "2. **Compute the Covariance Matrix**:\n",
    "\n",
    "\\[\n",
    "\\text{Covariance Matrix} = \\begin{pmatrix}\n",
    "0.61655556 & 0.61544444 & 0.40222222 \\\\\n",
    "0.61544444 & 0.71655556 & 0.48944444 \\\\\n",
    "0.40222222 & 0.48944444 & 0.34555556 \\\\\n",
    "\\end{pmatrix}\n",
    "\\]\n",
    "\n",
    "3. **Eigenvalue Decomposition**:\n",
    "\n",
    "\\[\n",
    "\\text{Eigenvalues} = \\begin{pmatrix}\n",
    "1.81242619 & 0.1555079 & 0.0100809 \\\\\n",
    "\\end{pmatrix}\n",
    "\\]\n",
    "\n",
    "\\[\n",
    "\\text{Eigenvectors} = \\begin{pmatrix}\n",
    "0.57735027 & 0.70710678 & -0.40824829 \\\\\n",
    "0.57735027 & -0.70710678 & -0.40824829 \\\\\n",
    "0.57735027 & 0 & 0.81649658 \\\\\n",
    "\\end{pmatrix}\n",
    "\\]\n",
    "\n",
    "4. **Select Principal Components**: Choose the eigenvectors corresponding to the largest eigenvalues. For example, we may choose the first two principal components.\n",
    "\n",
    "\\[\n",
    "\\text{Principal Components} = \\begin{pmatrix}\n",
    "0.57735027 & 0.70710678 \\\\\n",
    "0.57735027 & -0.70710678 \\\\\n",
    "0.57735027 & 0 \\\\\n",
    "\\end{pmatrix}\n",
    "\\]\n",
    "\n",
    "5. **Project the Data**:\n",
    "\n",
    "\\[\n",
    "\\text{Projected Data} = \\text{Standardized Data} \\times \\text{Principal Components}\n",
    "\\]\n",
    "\n",
    "This results in a new dataset with fewer dimensions (features) but still captures most of the variance:\n",
    "\n",
    "\\[\n",
    "\\text{Projected Data} = \\begin{pmatrix}\n",
    "0.87705802 & 0.15430335 \\\\\n",
    "-1.1799626 & -0.15601837 \\\\\n",
    "1.09689468 & -0.44965859 \\\\\n",
    "0.26478199 & -0.00833335 \\\\\n",
    "1.66195019 & 0.29465889 \\\\\n",
    "1.00626217 & 0.15416689 \\\\\n",
    "-0.3864285 & -0.40224224 \\\\\n",
    "-1.48205408 & -0.40175648 \\\\\n",
    "-0.80967948 & -0.38611561 \\\\\n",
    "-1.04882288 & -0.4989942 \\\\\n",
    "\\end{pmatrix}\n",
    "\\]\n",
    "\n",
    "### Applications of PCA for Feature Extraction\n",
    "\n",
    "1. **Data Compression**: Reducing the number of features while maintaining the essential structure of the data.\n",
    "2. **Noise Reduction**: By retaining only the principal components with the most variance, less significant components (often noise) are discarded.\n",
    "3. **Visualization**: Transforming high-dimensional data into 2D or 3D principal component space for easier visualization and interpretation.\n",
    "4. **Improved Performance**: Enhancing the performance of machine learning algorithms by reducing dimensionality and avoiding the curse of dimensionality.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7becf9f5-c39b-4b53-9ad4-0438ea6f1192",
   "metadata": {},
   "source": [
    "Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset\n",
    "contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to\n",
    "preprocess the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9d4d80-b048-4115-9b87-8da21c5103f9",
   "metadata": {},
   "source": [
    "To build a recommendation system for a food delivery service using features such as price, rating, and delivery time, preprocessing the data with Min-Max scaling ensures that all features contribute equally to the model. This prevents features with larger numerical ranges from dominating the model's learning process. Here’s a step-by-step guide on how to use Min-Max scaling for preprocessing the data:\n",
    "\n",
    "### Step-by-Step Guide to Using Min-Max Scaling\n",
    "\n",
    "1. **Understand the Dataset**:\n",
    "   - Identify the features to be scaled: price, rating, and delivery time.\n",
    "\n",
    "2. **Calculate Minimum and Maximum Values**:\n",
    "   - For each feature, calculate the minimum (\\(X_{\\text{min}}\\)) and maximum (\\(X_{\\text{max}}\\)) values in the dataset.\n",
    "\n",
    "3. **Apply Min-Max Scaling**:\n",
    "   - Use the Min-Max scaling formula to transform each feature value:\n",
    "     \\[\n",
    "     X_{\\text{scaled}} = \\frac{X - X_{\\text{min}}}{X_{\\text{max}} - X_{\\text{min}}}\n",
    "     \\]\n",
    "\n",
    "### Example Dataset\n",
    "\n",
    "Suppose we have the following dataset:\n",
    "\n",
    "\\[\n",
    "\\begin{array}{ccc}\n",
    "\\text{Price} & \\text{Rating} & \\text{Delivery Time (min)} \\\\\n",
    "\\hline\n",
    "10 & 4.5 & 30 \\\\\n",
    "15 & 4.7 & 25 \\\\\n",
    "20 & 4.0 & 45 \\\\\n",
    "25 & 3.5 & 50 \\\\\n",
    "30 & 4.8 & 20 \\\\\n",
    "\\end{array}\n",
    "\\]\n",
    "\n",
    "### Steps to Scale the Data\n",
    "\n",
    "1. **Identify Minimum and Maximum Values**:\n",
    "\n",
    "\\[\n",
    "\\begin{align*}\n",
    "\\text{Price}_{\\text{min}} &= 10, & \\text{Price}_{\\text{max}} &= 30 \\\\\n",
    "\\text{Rating}_{\\text{min}} &= 3.5, & \\text{Rating}_{\\text{max}} &= 4.8 \\\\\n",
    "\\text{Delivery Time}_{\\text{min}} &= 20, & \\text{Delivery Time}_{\\text{max}} &= 50 \\\\\n",
    "\\end{align*}\n",
    "\\]\n",
    "\n",
    "2. **Apply Min-Max Scaling Formula**:\n",
    "\n",
    "\\[\n",
    "\\begin{align*}\n",
    "\\text{Price}_{\\text{scaled}} &= \\frac{\\text{Price} - 10}{30 - 10} \\\\\n",
    "\\text{Rating}_{\\text{scaled}} &= \\frac{\\text{Rating} - 3.5}{4.8 - 3.5} \\\\\n",
    "\\text{Delivery Time}_{\\text{scaled}} &= \\frac{\\text{Delivery Time} - 20}{50 - 20} \\\\\n",
    "\\end{align*}\n",
    "\\]\n",
    "\n",
    "3. **Scale Each Value**:\n",
    "\n",
    "\\[\n",
    "\\begin{array}{ccc}\n",
    "\\text{Price}_{\\text{scaled}} & \\text{Rating}_{\\text{scaled}} & \\text{Delivery Time}_{\\text{scaled}} \\\\\n",
    "\\hline\n",
    "\\frac{10 - 10}{20} = 0 & \\frac{4.5 - 3.5}{1.3} = 0.769 & \\frac{30 - 20}{30} = 0.333 \\\\\n",
    "\\frac{15 - 10}{20} = 0.25 & \\frac{4.7 - 3.5}{1.3} = 0.923 & \\frac{25 - 20}{30} = 0.167 \\\\\n",
    "\\frac{20 - 10}{20} = 0.5 & \\frac{4.0 - 3.5}{1.3} = 0.385 & \\frac{45 - 20}{30} = 0.833 \\\\\n",
    "\\frac{25 - 10}{20} = 0.75 & \\frac{3.5 - 3.5}{1.3} = 0 & \\frac{50 - 20}{30} = 1 \\\\\n",
    "\\frac{30 - 10}{20} = 1 & \\frac{4.8 - 3.5}{1.3} = 1 & \\frac{20 - 20}{30} = 0 \\\\\n",
    "\\end{array}\n",
    "\\]\n",
    "\n",
    "### Scaled Dataset\n",
    "\n",
    "\\[\n",
    "\\begin{array}{ccc}\n",
    "\\text{Price}_{\\text{scaled}} & \\text{Rating}_{\\text{scaled}} & \\text{Delivery Time}_{\\text{scaled}} \\\\\n",
    "\\hline\n",
    "0 & 0.769 & 0.333 \\\\\n",
    "0.25 & 0.923 & 0.167 \\\\\n",
    "0.5 & 0.385 & 0.833 \\\\\n",
    "0.75 & 0 & 1 \\\\\n",
    "1 & 1 & 0 \\\\\n",
    "\\end{array}\n",
    "\\]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a9f7c5-2032-4128-bb41-5e123e364098",
   "metadata": {},
   "source": [
    "Q6. You are working on a project to build a model to predict stock prices. The dataset contains many\n",
    "features, such as company financial data and market trends. Explain how you would use PCA to reduce the\n",
    "dimensionality of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d0eaa9-f9f9-4e48-80f3-4d0a89c11367",
   "metadata": {},
   "source": [
    "Using Principal Component Analysis (PCA) to reduce the dimensionality of a dataset in a stock price prediction project involves transforming the original features into a smaller set of uncorrelated features that capture the most variance. This can help in simplifying the model, reducing computation time, and potentially improving the model's performance by eliminating noise and redundant information.\n",
    "\n",
    "### Steps to Use PCA for Dimensionality Reduction\n",
    "\n",
    "1. **Data Preparation**:\n",
    "   - **Collect the Dataset**: Ensure you have a comprehensive dataset with features like company financial data (e.g., earnings, revenue, debt) and market trends (e.g., stock indices, trading volume).\n",
    "   - **Handle Missing Values**: Impute or remove missing values to ensure a complete dataset.\n",
    "   - **Standardize the Data**: Standardize the features to have zero mean and unit variance because PCA is sensitive to the scale of the data.\n",
    "\n",
    "2. **Compute the Covariance Matrix**:\n",
    "   - Calculate the covariance matrix of the standardized data to understand how the features vary with respect to each other.\n",
    "\n",
    "3. **Eigenvalue Decomposition**:\n",
    "   - Compute the eigenvalues and eigenvectors of the covariance matrix. The eigenvalues indicate the amount of variance captured by each principal component, and the eigenvectors represent the directions of these components.\n",
    "\n",
    "4. **Select Principal Components**:\n",
    "   - Sort the eigenvalues in descending order and choose the top \\( k \\) eigenvectors corresponding to the largest eigenvalues. These top \\( k \\) eigenvectors form the new feature space.\n",
    "\n",
    "5. **Project the Data**:\n",
    "   - Transform the original dataset by projecting it onto the new feature space formed by the selected principal components. This results in a new dataset with reduced dimensionality.\n",
    "\n",
    "6. **Train the Model**:\n",
    "   - Use the reduced dataset to train your stock price prediction model. This can help in reducing overfitting and improving generalization.\n",
    "\n",
    "### Example of Applying PCA\n",
    "\n",
    "Suppose you have a dataset with 10 features:\n",
    "\n",
    "\\[\n",
    "\\begin{pmatrix}\n",
    "\\text{Feature 1} & \\text{Feature 2} & \\cdots & \\text{Feature 10} \\\\\n",
    "\\hline\n",
    "2.5 & 2.4 & \\cdots & 1.2 \\\\\n",
    "0.5 & 0.7 & \\cdots & 0.8 \\\\\n",
    "2.2 & 2.9 & \\cdots & 1.7 \\\\\n",
    "1.9 & 2.2 & \\cdots & 1.1 \\\\\n",
    "\\cdots & \\cdots & \\cdots & \\cdots \\\\\n",
    "\\end{pmatrix}\n",
    "\\]\n",
    "\n",
    "### Steps in Detail\n",
    "\n",
    "1. **Standardize the Data**:\n",
    "\n",
    "\\[\n",
    "\\text{Standardized Data} = \\frac{X - \\mu}{\\sigma}\n",
    "\\]\n",
    "\n",
    "2. **Compute the Covariance Matrix**:\n",
    "\n",
    "\\[\n",
    "\\text{Covariance Matrix} = \\frac{1}{n-1} \\sum_{i=1}^{n} (X_i - \\mu)(X_i - \\mu)^T\n",
    "\\]\n",
    "\n",
    "3. **Eigenvalue Decomposition**:\n",
    "\n",
    "\\[\n",
    "\\text{Covariance Matrix} = V \\Lambda V^T\n",
    "\\]\n",
    "\n",
    "Where \\( \\Lambda \\) is the diagonal matrix of eigenvalues, and \\( V \\) is the matrix of eigenvectors.\n",
    "\n",
    "4. **Select Principal Components**:\n",
    "   - Choose the top \\( k \\) eigenvectors. For instance, if you want to reduce from 10 features to 3 principal components, select the eigenvectors corresponding to the 3 largest eigenvalues.\n",
    "\n",
    "5. **Project the Data**:\n",
    "\n",
    "\\[\n",
    "\\text{Projected Data} = X \\times V_k\n",
    "\\]\n",
    "\n",
    "Where \\( V_k \\) contains the top \\( k \\) eigenvectors.\n",
    "\n",
    "### Benefits of Using PCA for Stock Price Prediction\n",
    "\n",
    "1. **Dimensionality Reduction**: Simplifies the model by reducing the number of features.\n",
    "2. **Noise Reduction**: Removes less important features that may add noise to the model.\n",
    "3. **Computational Efficiency**: Reduces the computational cost of training the model with fewer features.\n",
    "4. **Improved Model Performance**: Potentially enhances model performance by focusing on the most significant patterns in the data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f74506-661e-421e-8dff-1637243b7930",
   "metadata": {},
   "source": [
    "Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the\n",
    "values to a range of -1 to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca765001-8882-40ba-97d3-be9bd069d054",
   "metadata": {},
   "source": [
    "To perform Min-Max scaling to transform the values [1, 5, 10, 15, 20] to a range of -1 to 1, follow these steps:\n",
    "\n",
    "1. **Identify the minimum and maximum values in the original dataset**:\n",
    "   - Minimum value (\\(X_{\\text{min}}\\)) = 1\n",
    "   - Maximum value (\\(X_{\\text{max}}\\)) = 20\n",
    "\n",
    "2. **Use the Min-Max scaling formula to transform the values**:\n",
    "   - The formula for Min-Max scaling to a range [a, b] is:\n",
    "     \\[\n",
    "     X_{\\text{scaled}} = a + \\frac{(X - X_{\\text{min}}) \\times (b - a)}{X_{\\text{max}} - X_{\\text{min}}}\n",
    "     \\]\n",
    "   - Here, \\(a = -1\\) and \\(b = 1\\).\n",
    "\n",
    "3. **Apply the formula to each value in the dataset**:\n",
    "\n",
    "   For \\(X = 1\\):\n",
    "   \\[\n",
    "   X_{\\text{scaled}} = -1 + \\frac{(1 - 1) \\times (1 - (-1))}{20 - 1} = -1\n",
    "   \\]\n",
    "\n",
    "   For \\(X = 5\\):\n",
    "   \\[\n",
    "   X_{\\text{scaled}} = -1 + \\frac{(5 - 1) \\times (1 - (-1))}{20 - 1} = -1 + \\frac{4 \\times 2}{19} = -1 + \\frac{8}{19} = -1 + 0.421 = -0.579\n",
    "   \\]\n",
    "\n",
    "   For \\(X = 10\\):\n",
    "   \\[\n",
    "   X_{\\text{scaled}} = -1 + \\frac{(10 - 1) \\times (1 - (-1))}{20 - 1} = -1 + \\frac{9 \\times 2}{19} = -1 + \\frac{18}{19} = -1 + 0.947 = -0.053\n",
    "   \\]\n",
    "\n",
    "   For \\(X = 15\\):\n",
    "   \\[\n",
    "   X_{\\text{scaled}} = -1 + \\frac{(15 - 1) \\times (1 - (-1))}{20 - 1} = -1 + \\frac{14 \\times 2}{19} = -1 + \\frac{28}{19} = -1 + 1.474 = 0.474\n",
    "   \\]\n",
    "\n",
    "   For \\(X = 20\\):\n",
    "   \\[\n",
    "   X_{\\text{scaled}} = -1 + \\frac{(20 - 1) \\times (1 - (-1))}{20 - 1} = -1 + \\frac{19 \\times 2}{19} = -1 + 2 = 1\n",
    "   \\]\n",
    "\n",
    "### Scaled Values\n",
    "\n",
    "- \\(X = 1 \\rightarrow X_{\\text{scaled}} = -1\\)\n",
    "- \\(X = 5 \\rightarrow X_{\\text{scaled}} = -0.579\\)\n",
    "- \\(X = 10 \\rightarrow X_{\\text{scaled}} = -0.053\\)\n",
    "- \\(X = 15 \\rightarrow X_{\\text{scaled}} = 0.474\\)\n",
    "- \\(X = 20 \\rightarrow X_{\\text{scaled}} = 1\\)\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "The transformed values using Min-Max scaling to a range of -1 to 1 are:\n",
    "\n",
    "\\[\n",
    "[-1, -0.579, -0.053, 0.474, 1]\n",
    "\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de260884-d37e-4dd2-80b5-216abfb762f5",
   "metadata": {},
   "source": [
    "Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform\n",
    "Feature Extraction using PCA. How many principal components would you choose to retain, and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8ce205-5648-4b74-91ff-390a564fcada",
   "metadata": {},
   "source": [
    "To perform feature extraction using PCA on a dataset with features such as height, weight, age, gender, and blood pressure, follow these steps:\n",
    "\n",
    "### Step-by-Step Guide\n",
    "\n",
    "1. **Prepare the Dataset**:\n",
    "   - Ensure that the dataset is complete, with no missing values.\n",
    "\n",
    "2. **Standardize the Data**:\n",
    "   - Standardize the features to have zero mean and unit variance because PCA is sensitive to the scale of the data.\n",
    "   - Convert categorical features like gender to numerical values (e.g., 0 for male, 1 for female).\n",
    "\n",
    "3. **Compute the Covariance Matrix**:\n",
    "   - Calculate the covariance matrix of the standardized data to understand the variance and how the features relate to each other.\n",
    "\n",
    "4. **Perform Eigenvalue Decomposition**:\n",
    "   - Compute the eigenvalues and eigenvectors of the covariance matrix. The eigenvalues indicate the amount of variance captured by each principal component.\n",
    "\n",
    "5. **Select Principal Components**:\n",
    "   - Determine the number of principal components to retain based on the explained variance.\n",
    "\n",
    "### Example Calculation and Selection\n",
    "\n",
    "Let’s assume you have a dataset with the following features standardized:\n",
    "\n",
    "\\[\n",
    "\\begin{pmatrix}\n",
    "\\text{Height} & \\text{Weight} & \\text{Age} & \\text{Gender} & \\text{Blood Pressure} \\\\\n",
    "\\hline\n",
    "\\end{pmatrix}\n",
    "\\]\n",
    "\n",
    "### Steps in Detail\n",
    "\n",
    "1. **Standardize the Data**:\n",
    "\n",
    "   Standardized data ensures each feature has zero mean and unit variance.\n",
    "\n",
    "2. **Compute the Covariance Matrix**:\n",
    "\n",
    "\\[\n",
    "\\text{Covariance Matrix} = \\frac{1}{n-1} \\sum_{i=1}^{n} (X_i - \\mu)(X_i - \\mu)^T\n",
    "\\]\n",
    "\n",
    "3. **Eigenvalue Decomposition**:\n",
    "\n",
    "\\[\n",
    "\\text{Covariance Matrix} = V \\Lambda V^T\n",
    "\\]\n",
    "\n",
    "Where \\( \\Lambda \\) is the diagonal matrix of eigenvalues, and \\( V \\) is the matrix of eigenvectors.\n",
    "\n",
    "4. **Select Principal Components**:\n",
    "\n",
    "### How to Decide the Number of Principal Components to Retain\n",
    "\n",
    "1. **Explained Variance**:\n",
    "   - Calculate the explained variance for each principal component:\n",
    "     \\[\n",
    "     \\text{Explained Variance} = \\frac{\\lambda_i}{\\sum_{j=1}^{d} \\lambda_j}\n",
    "     \\]\n",
    "     Where \\( \\lambda_i \\) is the eigenvalue of the \\(i\\)-th principal component.\n",
    "\n",
    "2. **Cumulative Explained Variance**:\n",
    "   - Calculate the cumulative explained variance to determine how many components capture a significant amount of variance (typically 95% or more).\n",
    "\n",
    "### Example Explained Variance Calculation\n",
    "\n",
    "Assume the eigenvalues are sorted in descending order and you have the following explained variances:\n",
    "\n",
    "\\[\n",
    "\\begin{array}{ccc}\n",
    "\\text{Principal Component} & \\text{Eigenvalue} & \\text{Explained Variance (\\%)} \\\\\n",
    "\\hline\n",
    "1 & 3.2 & 64\\% \\\\\n",
    "2 & 1.0 & 20\\% \\\\\n",
    "3 & 0.7 & 14\\% \\\\\n",
    "4 & 0.05 & 1\\% \\\\\n",
    "5 & 0.05 & 1\\% \\\\\n",
    "\\end{array}\n",
    "\\]\n",
    "\n",
    "The cumulative explained variance is:\n",
    "\n",
    "\\[\n",
    "\\begin{array}{ccc}\n",
    "\\text{Principal Component} & \\text{Cumulative Explained Variance (\\%)} \\\\\n",
    "\\hline\n",
    "1 & 64\\% \\\\\n",
    "1 + 2 & 84\\% \\\\\n",
    "1 + 2 + 3 & 98\\% \\\\\n",
    "1 + 2 + 3 + 4 & 99\\% \\\\\n",
    "1 + 2 + 3 + 4 + 5 & 100\\% \\\\\n",
    "\\end{array}\n",
    "\\]\n",
    "\n",
    "### Decision on the Number of Principal Components\n",
    "\n",
    "- To capture at least 95% of the variance, you would retain the first three principal components since they together explain 98% of the variance.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "To perform PCA and decide on the number of principal components to retain for the dataset with features like height, weight, age, gender, and blood pressure, you:\n",
    "\n",
    "1. Standardize the data.\n",
    "2. Compute the covariance matrix.\n",
    "3. Perform eigenvalue decomposition.\n",
    "4. Calculate the explained variance for each principal component.\n",
    "5. Retain the number of principal components that together capture a significant portion (typically 95% or more) of the total variance.\n",
    "\n",
    "In this example, you would retain three principal components, as they explain 98% of the variance, ensuring that most of the information in the original dataset is preserved while reducing dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c173123-1a76-4316-86f1-8ca5335f7937",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
